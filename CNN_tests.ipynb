{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christof/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/christof/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing\n",
      "lowercase\n",
      "removing breaks\n",
      "expanding contractions\n",
      "replacing smileys\n",
      "replacing ip\n",
      "removing links\n",
      "replacing numbers\n",
      "removing bigrams\n",
      "isolating punct\n",
      "preprocessing\n",
      "lowercase\n",
      "removing breaks\n",
      "expanding contractions\n",
      "replacing smileys\n",
      "replacing ip\n",
      "removing links\n",
      "replacing numbers\n",
      "removing bigrams\n",
      "isolating punct\n",
      "getting embeddings\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import re, os, gc, time, pandas as pd, numpy as np\n",
    "import tqdm\n",
    "\n",
    "np.random.seed(32)\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"5\"\n",
    "from nltk import tokenize, word_tokenize\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, Add, Flatten, TimeDistributed,CuDNNGRU,CuDNNLSTM\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "# from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec, Layer\n",
    "from preprocess_utils import preprocess\n",
    "from global_variables import TRAIN_FILENAME, TEST_FILENAME, COMMENT, LIST_CLASSES, UNKNOWN_WORD\n",
    "import logging\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "   \n",
    "embed_size = 300\n",
    "max_features = 150000\n",
    "max_text_len = 300\n",
    "\n",
    "# EMBEDDING_FILE = \"../input/glove840b300dtxt/glove.840B.300d.txt\n",
    "EMBEDDING_FILE = \"assets/embedding_models/ft_300d_crawl/crawl-300d-2M.vec\"\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch + 1, score))\n",
    "\n",
    "def rm_hyperlinks(words):\n",
    "    words = [w if not (w.startswith('http') or\n",
    "                       w.startswith('www') or\n",
    "                       w.endswith('.com') or\n",
    "                        w.startswith('en.wikipedia.org/')) else 'url' for w in words]\n",
    "    return words\n",
    "\n",
    "def strip_spaces(words):\n",
    "    return [w.replace(' ', '') for w in words]\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    twitter_tokenizer = TweetTokenizer()\n",
    "    tokenized_sentences = []\n",
    "    for sentence in tqdm.tqdm(sentences,mininterval=5):\n",
    "        if hasattr(sentence, \"decode\"):\n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "        tokens = twitter_tokenizer.tokenize(sentence)\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "def tokenize_list_of_sentences(list_of_sentences):\n",
    "\n",
    "    list_of_tokenized_sentences = []\n",
    "    for sentences in list_of_sentences:\n",
    "        tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "        # more preprocess on word level\n",
    "        tokenized_sentences = [rm_hyperlinks(s) for s in tokenized_sentences]\n",
    "        tokenized_sentences = [strip_spaces(s) for s in tokenized_sentences]\n",
    "        list_of_tokenized_sentences.append(tokenized_sentences)\n",
    "\n",
    "    return list_of_tokenized_sentences\n",
    "\n",
    "def create_word2id(list_of_tokenized_sentences,max_features):\n",
    "    word_counter = Counter()\n",
    "    print('CREATING VOCABULARY')\n",
    "    for tokenized_sentences in list_of_tokenized_sentences:\n",
    "        for tokens in tqdm.tqdm(tokenized_sentences):\n",
    "            word_counter.update(tokens)\n",
    "\n",
    "    raw_counts = word_counter.most_common(max_features)\n",
    "    vocab = [char_tuple[0] for char_tuple in raw_counts]\n",
    "    print('%s words detected, keeping %s words' % (len(word_counter), len(vocab)))\n",
    "    word2id = {word: (ind + 1) for ind, word in enumerate(vocab)}\n",
    "    word2id[UNKNOWN_WORD] = len(word2id)\n",
    "    id2word = dict((id, word) for word, id in word2id.items())\n",
    "    return word2id, id2word\n",
    "\n",
    "def tokenized_sentences2seq(tokenized_sentences, words_dict):\n",
    "    print('converting to sequence')\n",
    "    sequences = []\n",
    "    for sentence in tqdm.tqdm(tokenized_sentences, mininterval=5):\n",
    "        seq = []\n",
    "        for token in sentence:\n",
    "            try:\n",
    "                seq.append(words_dict[token])\n",
    "            except KeyError:\n",
    "                seq.append(words_dict[UNKNOWN_WORD])\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "def tokenized_sentences2seq2(tokenized_sentences, words_dict):\n",
    "    print('converting to sequence')\n",
    "    sequences = [words_dict[token] if token in words_dict else words_dict[UNKNOWN_WORD] for token in tqdm.tqdm(tokenized_sentences, mininterval=5)]\n",
    "    return sequences\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def convert_tokens_to_ids(tokenized_sentences, embedding_word_dict, id2word):\n",
    "    words_train = []\n",
    "    'converting word index to embedding index'\n",
    "    for sentence in tqdm.tqdm(tokenized_sentences):\n",
    "        current_words = []\n",
    "        for word_index in sentence:\n",
    "            try:\n",
    "                word = id2word[word_index]\n",
    "                word_id = embedding_word_dict.get(word, len(embedding_word_dict) - 2)\n",
    "            except KeyError:\n",
    "                word_id = embedding_word_dict.get(UNKNOWN_WORD, len(embedding_word_dict) - 2)\n",
    "            current_words.append(word_id)\n",
    "\n",
    "        if len(current_words) >= max_text_len:\n",
    "            current_words = current_words[:max_text_len]\n",
    "        else:\n",
    "            current_words += [len(embedding_word_dict) - 1] * (max_text_len - len(current_words))\n",
    "        words_train.append(current_words)\n",
    "    return words_train\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_FILENAME)\n",
    "test_data = pd.read_csv(TEST_FILENAME)\n",
    "Y = train_data[LIST_CLASSES].values\n",
    "\n",
    "test_data = preprocess(test_data)\n",
    "train_data = preprocess(train_data)\n",
    "\n",
    "train_data = train_data[\"comment_text\"].fillna(\"fillna\").values\n",
    "test_data = test_data[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print('fitting tokenizer')\n",
    "tokenizer.fit_on_texts(list(train_data) + list(test_data))\n",
    "train_data = tokenizer.texts_to_sequences(train_data)\n",
    "test_data = tokenizer.texts_to_sequences(test_data)\n",
    "X = sequence.pad_sequences(train_data, maxlen=max_text_len)\n",
    "X_test = sequence.pad_sequences(test_data, maxlen=max_text_len)\n",
    "\n",
    "del train_data\n",
    "del test_data\n",
    "\n",
    "print('getting embeddings')\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     45000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 300, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 300, 64)      19264       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 299, 64)      38464       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 298, 64)      57664       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 296, 64)      96064       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 64)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 64)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 64)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 64)        0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 64)        0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_1 (A (None, 64)           64          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           attention_weighted_average_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            390         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 45,211,910\n",
      "Trainable params: 211,910\n",
      "Non-trainable params: 45,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import SpatialDropout1D, MaxPool1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Reshape, Conv2D, MaxPool2D\n",
    "from keras import regularizers\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def build_model(lr=0.0):\n",
    "    inp = Input(shape=(max_text_len, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    \n",
    "    conv_0 = Conv1D(64, kernel_size=1, kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_1 = Conv1D(64, kernel_size=2, kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_2 = Conv1D(64, kernel_size=3, kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_3 = Conv1D(64, kernel_size=5, kernel_initializer='normal',activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool1D(pool_size=(max_text_len - 1 + 1))(conv_0)\n",
    "    maxpool_1 = MaxPool1D(pool_size=(max_text_len - 2 + 1))(conv_1)\n",
    "    maxpool_2 = MaxPool1D(pool_size=(max_text_len - 3 + 1))(conv_2)\n",
    "    maxpool_3 = MaxPool1D(pool_size=(max_text_len - 5 + 1))(conv_3)\n",
    "        \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    z = AttentionWeightedAverage()(z)\n",
    "    z = Dropout(0.3)(z)\n",
    "        \n",
    "    out = Dense(6, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr), metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(lr=1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9671\n",
      " ROC-AUC - epoch: 1 - score: 0.978514\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04973, saving model to CNN_test1_0_.hdf5\n",
      "143614/143614 [==============================] - 30s 210us/step - loss: 0.0967 - acc: 0.9672 - val_loss: 0.0497 - val_acc: 0.9807\n",
      "Epoch 2/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9808\n",
      " ROC-AUC - epoch: 2 - score: 0.984996\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04973 to 0.04528, saving model to CNN_test1_0_.hdf5\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0525 - acc: 0.9808 - val_loss: 0.0453 - val_acc: 0.9824\n",
      "Epoch 3/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9818\n",
      " ROC-AUC - epoch: 3 - score: 0.985561\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04528 to 0.04289, saving model to CNN_test1_0_.hdf5\n",
      "143614/143614 [==============================] - 28s 195us/step - loss: 0.0480 - acc: 0.9818 - val_loss: 0.0429 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9827\n",
      " ROC-AUC - epoch: 4 - score: 0.987053\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 193us/step - loss: 0.0451 - acc: 0.9827 - val_loss: 0.0447 - val_acc: 0.9826\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9833\n",
      " ROC-AUC - epoch: 5 - score: 0.987044\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04289 to 0.04196, saving model to CNN_test1_0_.hdf5\n",
      "143614/143614 [==============================] - 28s 197us/step - loss: 0.0434 - acc: 0.9833 - val_loss: 0.0420 - val_acc: 0.9834\n",
      "Epoch 6/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837\n",
      " ROC-AUC - epoch: 6 - score: 0.986786\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04196 to 0.04092, saving model to CNN_test1_0_.hdf5\n",
      "143614/143614 [==============================] - 28s 196us/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0409 - val_acc: 0.9842\n",
      "Epoch 7/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9841\n",
      " ROC-AUC - epoch: 7 - score: 0.987241\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 193us/step - loss: 0.0406 - acc: 0.9841 - val_loss: 0.0429 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9844\n",
      " ROC-AUC - epoch: 8 - score: 0.987176\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 196us/step - loss: 0.0395 - acc: 0.9844 - val_loss: 0.0413 - val_acc: 0.9836\n",
      "Epoch 9/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9847\n",
      " ROC-AUC - epoch: 9 - score: 0.986959\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 198us/step - loss: 0.0387 - acc: 0.9847 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "Epoch 10/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9851\n",
      " ROC-AUC - epoch: 10 - score: 0.987723\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 198us/step - loss: 0.0376 - acc: 0.9851 - val_loss: 0.0418 - val_acc: 0.9832\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "fold_size = len(X) // 10\n",
    "for fold_id in range(0, fold_count):\n",
    "    fold_start = fold_size * fold_id\n",
    "    fold_end = fold_start + fold_size\n",
    "\n",
    "    if fold_id == 9:\n",
    "        fold_end = len(X)\n",
    "\n",
    "    X_valid = X[fold_start:fold_end]\n",
    "    Y_valid = Y[fold_start:fold_end]\n",
    "    X_train = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "    Y_train = np.concatenate([Y[:fold_start], Y[fold_end:]])\n",
    "\n",
    "    model = build_model(lr = 0.001)\n",
    "    file_path = \"CNN_test1_%s_.hdf5\" %fold_id\n",
    "    ra_val = RocAucEvaluation(validation_data = (X_valid, Y_valid), interval = 1)\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "    history = model.fit(X_train, Y_train, batch_size = 256, epochs = 10, validation_data = (X_valid, Y_valid),\n",
    "                  verbose = 1, callbacks = [ra_val, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     45000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 300, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 300, 64)      19264       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 299, 64)      38464       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 298, 64)      57664       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 296, 64)      96064       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 64)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 64)        0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 64)        0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 64)        0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 64)        0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_2 (A (None, 64)           64          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           attention_weighted_average_2[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            390         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 45,211,910\n",
      "Trainable params: 211,910\n",
      "Non-trainable params: 45,000,000\n",
      "__________________________________________________________________________________________________\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9689\n",
      " ROC-AUC - epoch: 1 - score: 0.975703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05006, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 29s 200us/step - loss: 0.0925 - acc: 0.9690 - val_loss: 0.0501 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9809\n",
      " ROC-AUC - epoch: 2 - score: 0.983514\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05006 to 0.04556, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 27s 191us/step - loss: 0.0528 - acc: 0.9809 - val_loss: 0.0456 - val_acc: 0.9821\n",
      "Epoch 3/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9818\n",
      " ROC-AUC - epoch: 3 - score: 0.985648\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04556 to 0.04389, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0484 - acc: 0.9818 - val_loss: 0.0439 - val_acc: 0.9826\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9825\n",
      " ROC-AUC - epoch: 4 - score: 0.985651\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04389 to 0.04324, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0432 - val_acc: 0.9827\n",
      "Epoch 5/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9831\n",
      " ROC-AUC - epoch: 5 - score: 0.986528\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04324 to 0.04273, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0443 - acc: 0.9831 - val_loss: 0.0427 - val_acc: 0.9831\n",
      "Epoch 6/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9835\n",
      " ROC-AUC - epoch: 6 - score: 0.987225\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 187us/step - loss: 0.0428 - acc: 0.9835 - val_loss: 0.0441 - val_acc: 0.9821\n",
      "Epoch 7/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9838\n",
      " ROC-AUC - epoch: 7 - score: 0.987319\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 186us/step - loss: 0.0413 - acc: 0.9838 - val_loss: 0.0429 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9841\n",
      " ROC-AUC - epoch: 8 - score: 0.987272\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0405 - acc: 0.9841 - val_loss: 0.0432 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9844\n",
      " ROC-AUC - epoch: 9 - score: 0.987804\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0427 - val_acc: 0.9829\n",
      "Epoch 10/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9846\n",
      " ROC-AUC - epoch: 10 - score: 0.987414\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0388 - acc: 0.9846 - val_loss: 0.0429 - val_acc: 0.9828\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9710\n",
      " ROC-AUC - epoch: 1 - score: 0.978755\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04703, saving model to Inception_ATT3_8_.hdf5\n",
      "143614/143614 [==============================] - 28s 198us/step - loss: 0.0907 - acc: 0.9710 - val_loss: 0.0470 - val_acc: 0.9824\n",
      "Epoch 2/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9809\n",
      " ROC-AUC - epoch: 2 - score: 0.982687\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04703 to 0.04463, saving model to Inception_ATT3_8_.hdf5\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0523 - acc: 0.9809 - val_loss: 0.0446 - val_acc: 0.9833\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9818\n",
      " ROC-AUC - epoch: 3 - score: 0.983637\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04463 to 0.04396, saving model to Inception_ATT3_8_.hdf5\n",
      "143614/143614 [==============================] - 28s 195us/step - loss: 0.0485 - acc: 0.9818 - val_loss: 0.0440 - val_acc: 0.9834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9823\n",
      " ROC-AUC - epoch: 4 - score: 0.984346\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 196us/step - loss: 0.0462 - acc: 0.9823 - val_loss: 0.0446 - val_acc: 0.9830\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9829\n",
      " ROC-AUC - epoch: 5 - score: 0.985450\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04396 to 0.04248, saving model to Inception_ATT3_8_.hdf5\n",
      "143614/143614 [==============================] - 29s 199us/step - loss: 0.0443 - acc: 0.9829 - val_loss: 0.0425 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9834\n",
      " ROC-AUC - epoch: 6 - score: 0.984942\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0430 - acc: 0.9834 - val_loss: 0.0441 - val_acc: 0.9830\n",
      "Epoch 7/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9836\n",
      " ROC-AUC - epoch: 7 - score: 0.984397\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 191us/step - loss: 0.0419 - acc: 0.9836 - val_loss: 0.0431 - val_acc: 0.9834\n",
      "Epoch 8/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9840\n",
      " ROC-AUC - epoch: 8 - score: 0.984919\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 188us/step - loss: 0.0409 - acc: 0.9840 - val_loss: 0.0425 - val_acc: 0.9838\n",
      "Epoch 9/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9842\n",
      " ROC-AUC - epoch: 9 - score: 0.984514\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 188us/step - loss: 0.0398 - acc: 0.9842 - val_loss: 0.0429 - val_acc: 0.9838\n",
      "Epoch 10/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9845\n",
      " ROC-AUC - epoch: 10 - score: 0.984691\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 191us/step - loss: 0.0392 - acc: 0.9845 - val_loss: 0.0430 - val_acc: 0.9842\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9675\n",
      " ROC-AUC - epoch: 1 - score: 0.974228\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05037, saving model to Inception_ATT3_9_.hdf5\n",
      "143613/143613 [==============================] - 28s 193us/step - loss: 0.0950 - acc: 0.9675 - val_loss: 0.0504 - val_acc: 0.9815\n",
      "Epoch 2/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9806\n",
      " ROC-AUC - epoch: 2 - score: 0.979520\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05037 to 0.04802, saving model to Inception_ATT3_9_.hdf5\n",
      "143613/143613 [==============================] - 27s 189us/step - loss: 0.0534 - acc: 0.9806 - val_loss: 0.0480 - val_acc: 0.9819\n",
      "Epoch 3/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9819\n",
      " ROC-AUC - epoch: 3 - score: 0.983194\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04802 to 0.04682, saving model to Inception_ATT3_9_.hdf5\n",
      "143613/143613 [==============================] - 28s 192us/step - loss: 0.0492 - acc: 0.9819 - val_loss: 0.0468 - val_acc: 0.9823\n",
      "Epoch 4/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9822\n",
      " ROC-AUC - epoch: 4 - score: 0.983627\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04682 to 0.04573, saving model to Inception_ATT3_9_.hdf5\n",
      "143613/143613 [==============================] - 28s 197us/step - loss: 0.0471 - acc: 0.9821 - val_loss: 0.0457 - val_acc: 0.9824\n",
      "Epoch 5/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9830\n",
      " ROC-AUC - epoch: 5 - score: 0.984350\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04573 to 0.04470, saving model to Inception_ATT3_9_.hdf5\n",
      "143613/143613 [==============================] - 28s 194us/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0447 - val_acc: 0.9829\n",
      "Epoch 6/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9833\n",
      " ROC-AUC - epoch: 6 - score: 0.985640\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 28s 192us/step - loss: 0.0435 - acc: 0.9833 - val_loss: 0.0448 - val_acc: 0.9825\n",
      "Epoch 7/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836\n",
      " ROC-AUC - epoch: 7 - score: 0.984798\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04470 to 0.04405, saving model to Inception_ATT3_9_.hdf5\n",
      "143613/143613 [==============================] - 28s 193us/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "Epoch 8/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9839\n",
      " ROC-AUC - epoch: 8 - score: 0.985571\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04405 to 0.04389, saving model to Inception_ATT3_9_.hdf5\n",
      "143613/143613 [==============================] - 28s 194us/step - loss: 0.0413 - acc: 0.9839 - val_loss: 0.0439 - val_acc: 0.9825\n",
      "Epoch 9/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9841\n",
      " ROC-AUC - epoch: 9 - score: 0.985008\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "143613/143613 [==============================] - 27s 191us/step - loss: 0.0404 - acc: 0.9841 - val_loss: 0.0444 - val_acc: 0.9830\n",
      "Epoch 10/10\n",
      "143104/143613 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9845\n",
      " ROC-AUC - epoch: 10 - score: 0.985121\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "143613/143613 [==============================] - 27s 191us/step - loss: 0.0393 - acc: 0.9845 - val_loss: 0.0444 - val_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "def build_model(lr=0.0):\n",
    "    inp = Input(shape=(max_text_len, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    \n",
    "    conv_0 = Conv1D(64, kernel_size=1, kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_1 = Conv1D(64, kernel_size=2, kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_2 = Conv1D(64, kernel_size=3, kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_3 = Conv1D(64, kernel_size=5, kernel_initializer='normal',activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool1D(pool_size=(max_text_len - 1 + 1))(conv_0)\n",
    "    maxpool_1 = MaxPool1D(pool_size=(max_text_len - 2 + 1))(conv_1)\n",
    "    maxpool_2 = MaxPool1D(pool_size=(max_text_len - 3 + 1))(conv_2)\n",
    "    maxpool_3 = MaxPool1D(pool_size=(max_text_len - 5 + 1))(conv_3)\n",
    "        \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
    "    z = AttentionWeightedAverage()(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "        \n",
    "    out = Dense(6, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr), metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(lr=1e-3)\n",
    "model.summary()\n",
    "\n",
    "fold_count = 10\n",
    "fold_size = len(X) // 10\n",
    "for fold_id in [7,8,9]:#range(0, fold_count):\n",
    "    fold_start = fold_size * fold_id\n",
    "    fold_end = fold_start + fold_size\n",
    "\n",
    "    if fold_id == 9:\n",
    "        fold_end = len(X)\n",
    "\n",
    "    X_valid = X[fold_start:fold_end]\n",
    "    Y_valid = Y[fold_start:fold_end]\n",
    "    X_train = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "    Y_train = np.concatenate([Y[:fold_start], Y[fold_end:]])\n",
    "\n",
    "    model = build_model(lr = 0.001)\n",
    "    file_path = \"Inception_ATT3_%s_.hdf5\" %fold_id\n",
    "    ra_val = RocAucEvaluation(validation_data = (X_valid, Y_valid), interval = 1)\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "    history = model.fit(X_train, Y_train, batch_size = 256, epochs = 10, validation_data = (X_valid, Y_valid),\n",
    "                  verbose = 1, callbacks = [ra_val, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 8s 53us/step\n",
      "15957/15957 [==============================] - 1s 53us/step\n",
      "153164/153164 [==============================] - 8s 54us/step\n",
      "15957/15957 [==============================] - 1s 53us/step\n",
      "153164/153164 [==============================] - 8s 54us/step\n",
      "15957/15957 [==============================] - 1s 53us/step\n",
      "153164/153164 [==============================] - 8s 55us/step\n",
      "15957/15957 [==============================] - 1s 54us/step\n",
      "153164/153164 [==============================] - 8s 55us/step\n",
      "15957/15957 [==============================] - 1s 54us/step\n",
      "153164/153164 [==============================] - 8s 55us/step\n",
      "15957/15957 [==============================] - 1s 54us/step\n",
      "153164/153164 [==============================] - 9s 56us/step\n",
      "15957/15957 [==============================] - 1s 54us/step\n",
      "153164/153164 [==============================] - 9s 56us/step\n",
      "15957/15957 [==============================] - 1s 54us/step\n",
      "153164/153164 [==============================] - 9s 56us/step\n",
      "15957/15957 [==============================] - 1s 54us/step\n",
      "153164/153164 [==============================] - 9s 56us/step\n",
      "15958/15958 [==============================] - 1s 55us/step\n"
     ]
    }
   ],
   "source": [
    "list_of_preds = []\n",
    "list_of_vals = []\n",
    "list_of_y = []\n",
    "fold_count = 10\n",
    "fold_size = len(X) // 10\n",
    "for fold_id in range(0, fold_count):\n",
    "    fold_start = fold_size * fold_id\n",
    "    fold_end = fold_start + fold_size\n",
    "\n",
    "    if fold_id == 9:\n",
    "        fold_end = len(X)\n",
    "\n",
    "    X_valid = X[fold_start:fold_end]\n",
    "    Y_valid = Y[fold_start:fold_end]\n",
    "    X_train = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "    Y_train = np.concatenate([Y[:fold_start], Y[fold_end:]])\n",
    "\n",
    "    file_path = 'Inception_ATT3_' + str(fold_id) + '_.hdf5'\n",
    "    model = load_model(file_path,custom_objects = {\"AttentionWeightedAverage\": AttentionWeightedAverage})\n",
    "    preds = model.predict(X_test, batch_size = 256, verbose = 1)\n",
    "    list_of_preds.append(preds)\n",
    "    vals = model.predict(X_valid, batch_size = 256, verbose = 1)\n",
    "    list_of_vals.append(vals)\n",
    "    list_of_y.append(Y_valid)\n",
    "test_predicts = np.zeros(list_of_preds[0].shape)\n",
    "for fold_predict in list_of_preds:\n",
    "    test_predicts += fold_predict\n",
    "\n",
    "test_predicts /= len(list_of_preds)\n",
    "submission = pd.read_csv('assets/raw_data/sample_submission.csv')\n",
    "submission[LIST_CLASSES] = test_predicts\n",
    "submission.to_csv('Inception_ATT3_l2_test_data.csv', index=False)\n",
    "\n",
    "l2_data = pd.DataFrame(columns=['logits_' + c for c in LIST_CLASSES]+LIST_CLASSES)\n",
    "l2_data[['logits_' + c for c in LIST_CLASSES]] = pd.DataFrame(np.concatenate(list_of_vals,axis = 0))\n",
    "l2_data[LIST_CLASSES] = pd.DataFrame(np.concatenate(list_of_y,axis = 0))\n",
    "l2_data.to_csv('Inception_ATT3_l2_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
