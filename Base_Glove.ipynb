{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christof/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/christof/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing\n",
      "lowercase\n",
      "removing breaks\n",
      "expanding contractions\n",
      "replacing smileys\n",
      "replacing ip\n",
      "removing links\n",
      "replacing numbers\n",
      "removing bigrams\n",
      "isolating punct\n",
      "preprocessing\n",
      "lowercase\n",
      "removing breaks\n",
      "expanding contractions\n",
      "replacing smileys\n",
      "replacing ip\n",
      "removing links\n",
      "replacing numbers\n",
      "removing bigrams\n",
      "isolating punct\n",
      "fitting tokenizer\n",
      "getting embeddings\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Input, Dense, Embedding, SpatialDropout1D, concatenate, PReLU\n",
    "from keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback, TensorBoard, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "import tensorflow as tf\n",
    "from global_variables import TRAIN_FILENAME, TEST_FILENAME, SAMPLE_SUBMISSION_FILENAME\n",
    "from preprocess_utils import preprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "max_features = 120000\n",
    "maxlen = 200\n",
    "\n",
    "\n",
    "#fix4->fix5:去掉concatenate后的Dropout层，SpatialDropout1D百分比0.45->0.5\n",
    "#fix5->fix6:modelcheckpoint\n",
    "#fix6->fix7:preprocess, gru后+prelu和dropout etc.\n",
    "#fix7->fix8:LR decrese, glove twitter200d\n",
    "#fix8->fix9:10fold\n",
    "#fix9->fix11:dropout0.5->0.2, attention layer\n",
    "\n",
    "def glove_preprocess(text):\n",
    "    #adapted from https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]?\"\n",
    "    text = re.sub(\"https?:* \", \"<URL>\", text)\n",
    "    text = re.sub(\"www.* \", \"<URL>\", text)\n",
    "    text = re.sub(\"\\[\\[User(.*)\\|\", '<USER>', text)\n",
    "    text = re.sub(\"<3\", '<HEART>', text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(eyes + nose + \"[Dd)]\", '<SMILE>', text)\n",
    "    text = re.sub(\"[(d]\" + nose + eyes, '<SMILE>', text)\n",
    "    text = re.sub(eyes + nose + \"p\", '<LOLFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"\\(\", '<SADFACE>', text)\n",
    "    text = re.sub(\"\\)\" + nose + eyes, '<SADFACE>', text)\n",
    "    text = re.sub(eyes + nose + \"[/|l*]\", '<NEUTRALFACE>', text)\n",
    "    text = re.sub(\"/\", \" / \", text)\n",
    "    text = re.sub(\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", text)\n",
    "    text = re.sub(\"([!]){2,}\", \"! <REPEAT>\", text)\n",
    "    text = re.sub(\"([?]){2,}\", \"? <REPEAT>\", text)\n",
    "    text = re.sub(\"([.]){2,}\", \". <REPEAT>\", text)\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    text = pattern.sub(r\"\\1\" + \" <ELONG>\", text)\n",
    "    return text\n",
    "\n",
    "def deduplicate(x, threshold):\n",
    "    word_list = x.split()\n",
    "    num_words = len(word_list)\n",
    "    if num_words == 0:\n",
    "        return x\n",
    "    else:\n",
    "        num_unique_words = len(set(word_list))\n",
    "        unique_ratio = num_words/num_unique_words\n",
    "        if unique_ratio > threshold:\n",
    "            x = ' '.join(x.split()[:num_unique_words])\n",
    "        return x\n",
    "\n",
    "train = pd.read_csv(TRAIN_FILENAME)\n",
    "test = pd.read_csv(TEST_FILENAME)\n",
    "submission = pd.read_csv(SAMPLE_SUBMISSION_FILENAME)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: glove_preprocess(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: glove_preprocess(x))\n",
    "\n",
    "train = preprocess(train)\n",
    "train = preprocess(train)\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\" \").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\" \").values\n",
    "\n",
    "tokenizer_w = text.Tokenizer(num_words=max_features)\n",
    "tokenizer_w.fit_on_texts(texts=list(X_train) + list(X_test))\n",
    "\n",
    "X_train = tokenizer_w.texts_to_sequences(X_train)\n",
    "X_test = tokenizer_w.texts_to_sequences(X_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "del X_train, X_test\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# EMBEDDING_FILE = '../embed/crawl-300d-2M.vec'\n",
    "EMBEDDING_FILE = 'assets/embedding_models/glove/glove.twitter.27B.200d.txt'\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE, encoding='UTF-8')))\n",
    "\n",
    "word_index = tokenizer_w.word_index \n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, idx in tqdm(word_index.items()):\n",
    "    if idx >= max_features: \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[idx] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
