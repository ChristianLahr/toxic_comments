{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christof/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/christof/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing\n",
      "lowercase\n",
      "removing breaks\n",
      "expanding contractions\n",
      "replacing smileys\n",
      "replacing ip\n",
      "removing links\n",
      "replacing numbers\n",
      "removing bigrams\n",
      "isolating punct\n",
      "preprocessing\n",
      "lowercase\n",
      "removing breaks\n",
      "expanding contractions\n",
      "replacing smileys\n",
      "replacing ip\n",
      "removing links\n",
      "replacing numbers\n",
      "removing bigrams\n",
      "isolating punct\n",
      "fitting tokenizer\n",
      "getting embeddings\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import re, os, gc, time, pandas as pd, numpy as np\n",
    "import tqdm\n",
    "\n",
    "np.random.seed(32)\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"5\"\n",
    "from nltk import tokenize, word_tokenize\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, Add, Flatten, TimeDistributed,CuDNNGRU,CuDNNLSTM\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "# from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec, Layer\n",
    "from preprocess_utils import preprocess\n",
    "from global_variables import TRAIN_FILENAME, TEST_FILENAME, COMMENT, LIST_CLASSES, UNKNOWN_WORD\n",
    "import logging\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "   \n",
    "embed_size = 300\n",
    "max_features = 150000\n",
    "max_text_len = 300\n",
    "\n",
    "# EMBEDDING_FILE = \"../input/glove840b300dtxt/glove.840B.300d.txt\n",
    "EMBEDDING_FILE = \"assets/embedding_models/ft_300d_crawl/crawl-300d-2M.vec\"\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch + 1, score))\n",
    "\n",
    "def rm_hyperlinks(words):\n",
    "    words = [w if not (w.startswith('http') or\n",
    "                       w.startswith('www') or\n",
    "                       w.endswith('.com') or\n",
    "                        w.startswith('en.wikipedia.org/')) else 'url' for w in words]\n",
    "    return words\n",
    "\n",
    "def strip_spaces(words):\n",
    "    return [w.replace(' ', '') for w in words]\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    twitter_tokenizer = TweetTokenizer()\n",
    "    tokenized_sentences = []\n",
    "    for sentence in tqdm.tqdm(sentences,mininterval=5):\n",
    "        if hasattr(sentence, \"decode\"):\n",
    "            sentence = sentence.decode(\"utf-8\")\n",
    "        tokens = twitter_tokenizer.tokenize(sentence)\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "def tokenize_list_of_sentences(list_of_sentences):\n",
    "\n",
    "    list_of_tokenized_sentences = []\n",
    "    for sentences in list_of_sentences:\n",
    "        tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "        # more preprocess on word level\n",
    "        tokenized_sentences = [rm_hyperlinks(s) for s in tokenized_sentences]\n",
    "        tokenized_sentences = [strip_spaces(s) for s in tokenized_sentences]\n",
    "        list_of_tokenized_sentences.append(tokenized_sentences)\n",
    "\n",
    "    return list_of_tokenized_sentences\n",
    "\n",
    "def create_word2id(list_of_tokenized_sentences,max_features):\n",
    "    word_counter = Counter()\n",
    "    print('CREATING VOCABULARY')\n",
    "    for tokenized_sentences in list_of_tokenized_sentences:\n",
    "        for tokens in tqdm.tqdm(tokenized_sentences):\n",
    "            word_counter.update(tokens)\n",
    "\n",
    "    raw_counts = word_counter.most_common(max_features)\n",
    "    vocab = [char_tuple[0] for char_tuple in raw_counts]\n",
    "    print('%s words detected, keeping %s words' % (len(word_counter), len(vocab)))\n",
    "    word2id = {word: (ind + 1) for ind, word in enumerate(vocab)}\n",
    "    word2id[UNKNOWN_WORD] = len(word2id)\n",
    "    id2word = dict((id, word) for word, id in word2id.items())\n",
    "    return word2id, id2word\n",
    "\n",
    "def tokenized_sentences2seq(tokenized_sentences, words_dict):\n",
    "    print('converting to sequence')\n",
    "    sequences = []\n",
    "    for sentence in tqdm.tqdm(tokenized_sentences, mininterval=5):\n",
    "        seq = []\n",
    "        for token in sentence:\n",
    "            try:\n",
    "                seq.append(words_dict[token])\n",
    "            except KeyError:\n",
    "                seq.append(words_dict[UNKNOWN_WORD])\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "def tokenized_sentences2seq2(tokenized_sentences, words_dict):\n",
    "    print('converting to sequence')\n",
    "    sequences = [words_dict[token] if token in words_dict else words_dict[UNKNOWN_WORD] for token in tqdm.tqdm(tokenized_sentences, mininterval=5)]\n",
    "    return sequences\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def convert_tokens_to_ids(tokenized_sentences, embedding_word_dict, id2word):\n",
    "    words_train = []\n",
    "    'converting word index to embedding index'\n",
    "    for sentence in tqdm.tqdm(tokenized_sentences):\n",
    "        current_words = []\n",
    "        for word_index in sentence:\n",
    "            try:\n",
    "                word = id2word[word_index]\n",
    "                word_id = embedding_word_dict.get(word, len(embedding_word_dict) - 2)\n",
    "            except KeyError:\n",
    "                word_id = embedding_word_dict.get(UNKNOWN_WORD, len(embedding_word_dict) - 2)\n",
    "            current_words.append(word_id)\n",
    "\n",
    "        if len(current_words) >= max_text_len:\n",
    "            current_words = current_words[:max_text_len]\n",
    "        else:\n",
    "            current_words += [len(embedding_word_dict) - 1] * (max_text_len - len(current_words))\n",
    "        words_train.append(current_words)\n",
    "    return words_train\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_FILENAME)\n",
    "test_data = pd.read_csv(TEST_FILENAME)\n",
    "Y = train_data[LIST_CLASSES].values\n",
    "\n",
    "test_data = preprocess(test_data)\n",
    "train_data = preprocess(train_data)\n",
    "\n",
    "train_data = train_data[\"comment_text\"].fillna(\"fillna\").values\n",
    "test_data = test_data[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print('fitting tokenizer')\n",
    "tokenizer.fit_on_texts(list(train_data) + list(test_data))\n",
    "train_data = tokenizer.texts_to_sequences(train_data)\n",
    "test_data = tokenizer.texts_to_sequences(test_data)\n",
    "X = sequence.pad_sequences(train_data, maxlen=max_text_len)\n",
    "X_test = sequence.pad_sequences(test_data, maxlen=max_text_len)\n",
    "\n",
    "del train_data\n",
    "del test_data\n",
    "\n",
    "print('getting embeddings')\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 300, 300)          45000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 300, 256)          329472    \n",
      "_________________________________________________________________\n",
      "capsule_4 (Capsule)          (None, 10, 16)            40960     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 45,371,398\n",
      "Trainable params: 371,398\n",
      "Non-trainable params: 45,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(lr=0.0):\n",
    "    inp = Input(shape=(max_text_len, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(rate_drop_dense)(x)\n",
    "    \n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        x)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(6, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(lr=1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9766\n",
      " ROC-AUC - epoch: 1 - score: 0.970651\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04887, saving model to CAPS_0_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0791 - acc: 0.9766 - val_loss: 0.0489 - val_acc: 0.9819\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9819\n",
      " ROC-AUC - epoch: 2 - score: 0.983306\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04887 to 0.04589, saving model to CAPS_0_.hdf5\n",
      "143614/143614 [==============================] - 313s 2ms/step - loss: 0.0495 - acc: 0.9819 - val_loss: 0.0459 - val_acc: 0.9822\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9827\n",
      " ROC-AUC - epoch: 3 - score: 0.987638\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04589 to 0.04526, saving model to CAPS_0_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0458 - acc: 0.9827 - val_loss: 0.0453 - val_acc: 0.9824\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9834\n",
      " ROC-AUC - epoch: 4 - score: 0.988615\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04526 to 0.04264, saving model to CAPS_0_.hdf5\n",
      "143614/143614 [==============================] - 313s 2ms/step - loss: 0.0435 - acc: 0.9833 - val_loss: 0.0426 - val_acc: 0.9830\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9771\n",
      " ROC-AUC - epoch: 1 - score: 0.967382\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05109, saving model to CAPS_1_.hdf5\n",
      "143614/143614 [==============================] - 316s 2ms/step - loss: 0.0798 - acc: 0.9771 - val_loss: 0.0511 - val_acc: 0.9811\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9819\n",
      " ROC-AUC - epoch: 2 - score: 0.983396\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05109 to 0.04627, saving model to CAPS_1_.hdf5\n",
      "143614/143614 [==============================] - 313s 2ms/step - loss: 0.0492 - acc: 0.9819 - val_loss: 0.0463 - val_acc: 0.9824\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9830\n",
      " ROC-AUC - epoch: 3 - score: 0.986735\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04627 to 0.04488, saving model to CAPS_1_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0452 - acc: 0.9830 - val_loss: 0.0449 - val_acc: 0.9825\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9835\n",
      " ROC-AUC - epoch: 4 - score: 0.988020\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04488 to 0.04331, saving model to CAPS_1_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0429 - acc: 0.9835 - val_loss: 0.0433 - val_acc: 0.9827\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9767\n",
      " ROC-AUC - epoch: 1 - score: 0.979377\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04847, saving model to CAPS_2_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0790 - acc: 0.9768 - val_loss: 0.0485 - val_acc: 0.9818\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9821\n",
      " ROC-AUC - epoch: 2 - score: 0.985380\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04847 to 0.04324, saving model to CAPS_2_.hdf5\n",
      "143614/143614 [==============================] - 311s 2ms/step - loss: 0.0488 - acc: 0.9821 - val_loss: 0.0432 - val_acc: 0.9833\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9827\n",
      " ROC-AUC - epoch: 3 - score: 0.988405\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04324 to 0.04243, saving model to CAPS_2_.hdf5\n",
      "143614/143614 [==============================] - 311s 2ms/step - loss: 0.0453 - acc: 0.9827 - val_loss: 0.0424 - val_acc: 0.9839\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9835\n",
      " ROC-AUC - epoch: 4 - score: 0.990226\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04243 to 0.04007, saving model to CAPS_2_.hdf5\n",
      "143614/143614 [==============================] - 312s 2ms/step - loss: 0.0432 - acc: 0.9835 - val_loss: 0.0401 - val_acc: 0.9842\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9771\n",
      " ROC-AUC - epoch: 1 - score: 0.964161\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04954, saving model to CAPS_3_.hdf5\n",
      "143614/143614 [==============================] - 317s 2ms/step - loss: 0.0776 - acc: 0.9771 - val_loss: 0.0495 - val_acc: 0.9820\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9820\n",
      " ROC-AUC - epoch: 2 - score: 0.976968\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04954 to 0.04610, saving model to CAPS_3_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0487 - acc: 0.9820 - val_loss: 0.0461 - val_acc: 0.9828\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9828\n",
      " ROC-AUC - epoch: 3 - score: 0.983975\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04610 to 0.04256, saving model to CAPS_3_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0454 - acc: 0.9828 - val_loss: 0.0426 - val_acc: 0.9836\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9834\n",
      " ROC-AUC - epoch: 4 - score: 0.985862\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0431 - acc: 0.9834 - val_loss: 0.0438 - val_acc: 0.9830\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9768\n",
      " ROC-AUC - epoch: 1 - score: 0.973149\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04881, saving model to CAPS_4_.hdf5\n",
      "143614/143614 [==============================] - 318s 2ms/step - loss: 0.0786 - acc: 0.9768 - val_loss: 0.0488 - val_acc: 0.9822\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9819\n",
      " ROC-AUC - epoch: 2 - score: 0.980702\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04881 to 0.04445, saving model to CAPS_4_.hdf5\n",
      "143614/143614 [==============================] - 315s 2ms/step - loss: 0.0494 - acc: 0.9819 - val_loss: 0.0444 - val_acc: 0.9832\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9825\n",
      " ROC-AUC - epoch: 3 - score: 0.983825\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04445 to 0.04224, saving model to CAPS_4_.hdf5\n",
      "143614/143614 [==============================] - 315s 2ms/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0422 - val_acc: 0.9838\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9832\n",
      " ROC-AUC - epoch: 4 - score: 0.984405\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0436 - acc: 0.9832 - val_loss: 0.0439 - val_acc: 0.9831\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9765\n",
      " ROC-AUC - epoch: 1 - score: 0.973980\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05347, saving model to CAPS_5_.hdf5\n",
      "143614/143614 [==============================] - 321s 2ms/step - loss: 0.0794 - acc: 0.9765 - val_loss: 0.0535 - val_acc: 0.9798\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9820\n",
      " ROC-AUC - epoch: 2 - score: 0.982700\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05347 to 0.04561, saving model to CAPS_5_.hdf5\n",
      "143614/143614 [==============================] - 317s 2ms/step - loss: 0.0495 - acc: 0.9820 - val_loss: 0.0456 - val_acc: 0.9825\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9828\n",
      " ROC-AUC - epoch: 3 - score: 0.985854\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04561 to 0.04076, saving model to CAPS_5_.hdf5\n",
      "143614/143614 [==============================] - 317s 2ms/step - loss: 0.0457 - acc: 0.9828 - val_loss: 0.0408 - val_acc: 0.9840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9834\n",
      " ROC-AUC - epoch: 4 - score: 0.987478\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 315s 2ms/step - loss: 0.0435 - acc: 0.9834 - val_loss: 0.0416 - val_acc: 0.9833\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9760\n",
      " ROC-AUC - epoch: 1 - score: 0.971764\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04677, saving model to CAPS_6_.hdf5\n",
      "143614/143614 [==============================] - 319s 2ms/step - loss: 0.0818 - acc: 0.9761 - val_loss: 0.0468 - val_acc: 0.9829\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9822\n",
      " ROC-AUC - epoch: 2 - score: 0.980268\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04677 to 0.04432, saving model to CAPS_6_.hdf5\n",
      "143614/143614 [==============================] - 315s 2ms/step - loss: 0.0488 - acc: 0.9822 - val_loss: 0.0443 - val_acc: 0.9830\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9827\n",
      " ROC-AUC - epoch: 3 - score: 0.985439\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04432 to 0.04197, saving model to CAPS_6_.hdf5\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0456 - acc: 0.9827 - val_loss: 0.0420 - val_acc: 0.9836\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9833\n",
      " ROC-AUC - epoch: 4 - score: 0.987194\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 314s 2ms/step - loss: 0.0433 - acc: 0.9833 - val_loss: 0.0422 - val_acc: 0.9835\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9769\n",
      " ROC-AUC - epoch: 1 - score: 0.969129\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05063, saving model to CAPS_7_.hdf5\n",
      "143614/143614 [==============================] - 318s 2ms/step - loss: 0.0803 - acc: 0.9769 - val_loss: 0.0506 - val_acc: 0.9810\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9820\n",
      " ROC-AUC - epoch: 2 - score: 0.981729\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05063 to 0.04528, saving model to CAPS_7_.hdf5\n",
      "143614/143614 [==============================] - 313s 2ms/step - loss: 0.0495 - acc: 0.9820 - val_loss: 0.0453 - val_acc: 0.9823\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9827\n",
      " ROC-AUC - epoch: 3 - score: 0.986280\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 312s 2ms/step - loss: 0.0457 - acc: 0.9827 - val_loss: 0.0467 - val_acc: 0.9810\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9835\n",
      " ROC-AUC - epoch: 4 - score: 0.987543\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04528 to 0.04206, saving model to CAPS_7_.hdf5\n",
      "143614/143614 [==============================] - 313s 2ms/step - loss: 0.0430 - acc: 0.9835 - val_loss: 0.0421 - val_acc: 0.9831\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9763\n",
      " ROC-AUC - epoch: 1 - score: 0.973437\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04932, saving model to CAPS_8_.hdf5\n",
      "143614/143614 [==============================] - 317s 2ms/step - loss: 0.0800 - acc: 0.9763 - val_loss: 0.0493 - val_acc: 0.9821\n",
      "Epoch 2/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9817\n",
      " ROC-AUC - epoch: 2 - score: 0.981109\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04932 to 0.04396, saving model to CAPS_8_.hdf5\n",
      "143614/143614 [==============================] - 312s 2ms/step - loss: 0.0499 - acc: 0.9817 - val_loss: 0.0440 - val_acc: 0.9834\n",
      "Epoch 3/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9826\n",
      " ROC-AUC - epoch: 3 - score: 0.984203\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04396 to 0.04324, saving model to CAPS_8_.hdf5\n",
      "143614/143614 [==============================] - 311s 2ms/step - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0432 - val_acc: 0.9833\n",
      "Epoch 4/4\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9832\n",
      " ROC-AUC - epoch: 4 - score: 0.987454\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04324 to 0.04120, saving model to CAPS_8_.hdf5\n",
      "143614/143614 [==============================] - 312s 2ms/step - loss: 0.0436 - acc: 0.9832 - val_loss: 0.0412 - val_acc: 0.9842\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9765\n",
      " ROC-AUC - epoch: 1 - score: 0.966827\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04991, saving model to CAPS_9_.hdf5\n",
      "143613/143613 [==============================] - 319s 2ms/step - loss: 0.0823 - acc: 0.9765 - val_loss: 0.0499 - val_acc: 0.9822\n",
      "Epoch 2/4\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9819\n",
      " ROC-AUC - epoch: 2 - score: 0.978684\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04991 to 0.04546, saving model to CAPS_9_.hdf5\n",
      "143613/143613 [==============================] - 313s 2ms/step - loss: 0.0492 - acc: 0.9819 - val_loss: 0.0455 - val_acc: 0.9828\n",
      "Epoch 3/4\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9829\n",
      " ROC-AUC - epoch: 3 - score: 0.986728\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "143613/143613 [==============================] - 312s 2ms/step - loss: 0.0455 - acc: 0.9829 - val_loss: 0.0456 - val_acc: 0.9820\n",
      "Epoch 4/4\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9834\n",
      " ROC-AUC - epoch: 4 - score: 0.988253\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04546 to 0.04241, saving model to CAPS_9_.hdf5\n",
      "143613/143613 [==============================] - 313s 2ms/step - loss: 0.0431 - acc: 0.9834 - val_loss: 0.0424 - val_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "fold_count = 10\n",
    "fold_size = len(X) // 10\n",
    "for fold_id in range(0, fold_count):\n",
    "    fold_start = fold_size * fold_id\n",
    "    fold_end = fold_start + fold_size\n",
    "\n",
    "    if fold_id == 9:\n",
    "        fold_end = len(X)\n",
    "\n",
    "    X_valid = X[fold_start:fold_end]\n",
    "    Y_valid = Y[fold_start:fold_end]\n",
    "    X_train = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "    Y_train = np.concatenate([Y[:fold_start], Y[fold_end:]])\n",
    "\n",
    "    model = build_model(lr = 0.001)\n",
    "    file_path = \"CAPS_%s_.hdf5\" %fold_id\n",
    "    ra_val = RocAucEvaluation(validation_data = (X_valid, Y_valid), interval = 1)\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "    history = model.fit(X_train, Y_train, batch_size = 256, epochs = 4, validation_data = (X_valid, Y_valid),\n",
    "                  verbose = 1, callbacks = [ra_val, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     45000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 300, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 300, 64)      19264       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 299, 64)      38464       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 298, 64)      57664       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 296, 64)      96064       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 64)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 64)        0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 64)        0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 64)        0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 64)        0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_2 (A (None, 64)           64          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           attention_weighted_average_2[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            390         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 45,211,910\n",
      "Trainable params: 211,910\n",
      "Non-trainable params: 45,000,000\n",
      "__________________________________________________________________________________________________\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9689\n",
      " ROC-AUC - epoch: 1 - score: 0.975703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05006, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 29s 200us/step - loss: 0.0925 - acc: 0.9690 - val_loss: 0.0501 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9809\n",
      " ROC-AUC - epoch: 2 - score: 0.983514\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05006 to 0.04556, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 27s 191us/step - loss: 0.0528 - acc: 0.9809 - val_loss: 0.0456 - val_acc: 0.9821\n",
      "Epoch 3/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9818\n",
      " ROC-AUC - epoch: 3 - score: 0.985648\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04556 to 0.04389, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0484 - acc: 0.9818 - val_loss: 0.0439 - val_acc: 0.9826\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9825\n",
      " ROC-AUC - epoch: 4 - score: 0.985651\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04389 to 0.04324, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0432 - val_acc: 0.9827\n",
      "Epoch 5/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9831\n",
      " ROC-AUC - epoch: 5 - score: 0.986528\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04324 to 0.04273, saving model to Inception_ATT3_7_.hdf5\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0443 - acc: 0.9831 - val_loss: 0.0427 - val_acc: 0.9831\n",
      "Epoch 6/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9835\n",
      " ROC-AUC - epoch: 6 - score: 0.987225\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 187us/step - loss: 0.0428 - acc: 0.9835 - val_loss: 0.0441 - val_acc: 0.9821\n",
      "Epoch 7/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9838\n",
      " ROC-AUC - epoch: 7 - score: 0.987319\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 27s 186us/step - loss: 0.0413 - acc: 0.9838 - val_loss: 0.0429 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9841\n",
      " ROC-AUC - epoch: 8 - score: 0.987272\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0405 - acc: 0.9841 - val_loss: 0.0432 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9844\n",
      " ROC-AUC - epoch: 9 - score: 0.987804\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 192us/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0427 - val_acc: 0.9829\n",
      "Epoch 10/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9846\n",
      " ROC-AUC - epoch: 10 - score: 0.987414\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0388 - acc: 0.9846 - val_loss: 0.0429 - val_acc: 0.9828\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9710\n",
      " ROC-AUC - epoch: 1 - score: 0.978755\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04703, saving model to Inception_ATT3_8_.hdf5\n",
      "143614/143614 [==============================] - 28s 198us/step - loss: 0.0907 - acc: 0.9710 - val_loss: 0.0470 - val_acc: 0.9824\n",
      "Epoch 2/10\n",
      "143104/143614 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9809\n",
      " ROC-AUC - epoch: 2 - score: 0.982687\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04703 to 0.04463, saving model to Inception_ATT3_8_.hdf5\n",
      "143614/143614 [==============================] - 28s 194us/step - loss: 0.0523 - acc: 0.9809 - val_loss: 0.0446 - val_acc: 0.9833\n",
      "Epoch 3/10\n",
      " 39936/143614 [=======>......................] - ETA: 18s - loss: 0.0483 - acc: 0.9820"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fold_count = 10\n",
    "fold_size = len(X) // 10\n",
    "for fold_id in [7,8,9]:#range(0, fold_count):\n",
    "    fold_start = fold_size * fold_id\n",
    "    fold_end = fold_start + fold_size\n",
    "\n",
    "    if fold_id == 9:\n",
    "        fold_end = len(X)\n",
    "\n",
    "    X_valid = X[fold_start:fold_end]\n",
    "    Y_valid = Y[fold_start:fold_end]\n",
    "    X_train = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "    Y_train = np.concatenate([Y[:fold_start], Y[fold_end:]])\n",
    "\n",
    "    model = build_model(lr = 0.001)\n",
    "    file_path = \"CAPS_%s_.hdf5\" %fold_id\n",
    "    ra_val = RocAucEvaluation(validation_data = (X_valid, Y_valid), interval = 1)\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "    history = model.fit(X_train, Y_train, batch_size = 256, epochs = 10, validation_data = (X_valid, Y_valid),\n",
    "                  verbose = 1, callbacks = [ra_val, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 92s 603us/step\n",
      "15957/15957 [==============================] - 10s 602us/step\n",
      "153164/153164 [==============================] - 93s 608us/step\n",
      "15957/15957 [==============================] - 10s 607us/step\n",
      "153164/153164 [==============================] - 94s 613us/step\n",
      "15957/15957 [==============================] - 10s 611us/step\n",
      "153164/153164 [==============================] - 94s 614us/step\n",
      "15957/15957 [==============================] - 10s 610us/step\n",
      "153164/153164 [==============================] - 93s 609us/step\n",
      "15957/15957 [==============================] - 10s 604us/step\n",
      "153164/153164 [==============================] - 94s 611us/step\n",
      "15957/15957 [==============================] - 10s 606us/step\n",
      "153164/153164 [==============================] - 95s 622us/step\n",
      "15957/15957 [==============================] - 10s 616us/step\n",
      "153164/153164 [==============================] - 94s 611us/step\n",
      "15957/15957 [==============================] - 10s 604us/step\n",
      "153164/153164 [==============================] - 95s 620us/step\n",
      "15957/15957 [==============================] - 10s 613us/step\n",
      "153164/153164 [==============================] - 95s 620us/step\n",
      "15958/15958 [==============================] - 10s 614us/step\n"
     ]
    }
   ],
   "source": [
    "list_of_preds = []\n",
    "list_of_vals = []\n",
    "list_of_y = []\n",
    "fold_count = 10\n",
    "fold_size = len(X) // 10\n",
    "for fold_id in range(0, fold_count):\n",
    "    fold_start = fold_size * fold_id\n",
    "    fold_end = fold_start + fold_size\n",
    "\n",
    "    if fold_id == 9:\n",
    "        fold_end = len(X)\n",
    "\n",
    "    X_valid = X[fold_start:fold_end]\n",
    "    Y_valid = Y[fold_start:fold_end]\n",
    "    X_train = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "    Y_train = np.concatenate([Y[:fold_start], Y[fold_end:]])\n",
    "\n",
    "    file_path = 'CAPS_' + str(fold_id) + '_.hdf5'\n",
    "    model = build_model(lr = 0.001)\n",
    "    model.load_weights(file_path)\n",
    "    #model = load_model(file_path,custom_objects = {\"Capsule\": Capsule})\n",
    "    preds = model.predict(X_test, batch_size = 256, verbose = 1)\n",
    "    list_of_preds.append(preds)\n",
    "    vals = model.predict(X_valid, batch_size = 256, verbose = 1)\n",
    "    list_of_vals.append(vals)\n",
    "    list_of_y.append(Y_valid)\n",
    "test_predicts = np.zeros(list_of_preds[0].shape)\n",
    "for fold_predict in list_of_preds:\n",
    "    test_predicts += fold_predict\n",
    "\n",
    "test_predicts /= len(list_of_preds)\n",
    "submission = pd.read_csv('assets/raw_data/sample_submission.csv')\n",
    "submission[LIST_CLASSES] = test_predicts\n",
    "submission.to_csv('CAPS_l2_test_data.csv', index=False)\n",
    "\n",
    "l2_data = pd.DataFrame(columns=['logits_' + c for c in LIST_CLASSES]+LIST_CLASSES)\n",
    "l2_data[['logits_' + c for c in LIST_CLASSES]] = pd.DataFrame(np.concatenate(list_of_vals,axis = 0))\n",
    "l2_data[LIST_CLASSES] = pd.DataFrame(np.concatenate(list_of_y,axis = 0))\n",
    "l2_data.to_csv('CAPS_l2_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
